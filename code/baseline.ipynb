{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1J1TvAJQqV4YsGepz3N_KWY-zWIrzQ-ls","timestamp":1709019759006}],"gpuType":"T4","authorship_tag":"ABX9TyPLk35YrJv24wPTFlY5tHlN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f8e60ab5271149cf83af910df003189e":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"success","description":"✔ Done","disabled":true,"icon":"","layout":"IPY_MODEL_f09330497ca94e2086adbfe9dbb8c254","style":"IPY_MODEL_f301fd2044954fc99b824f4a841c7e2a","tooltip":""}},"f09330497ca94e2086adbfe9dbb8c254":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":"50px","object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f301fd2044954fc99b824f4a841c7e2a":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"12776edbe7cc4f088574e8be133dd831":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"success","description":"✔ Done","disabled":true,"icon":"","layout":"IPY_MODEL_87c9f3c8b7254aeead1722fbf430d238","style":"IPY_MODEL_066a11b6ba974aaca5799b28fa10d94c","tooltip":""}},"87c9f3c8b7254aeead1722fbf430d238":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":"50px","object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"066a11b6ba974aaca5799b28fa10d94c":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}}}}},"cells":[{"cell_type":"markdown","source":["# **영수증 글자 검출 Baseline code Tutorial**\n","\n","\n","> 영수증 글자 검출 대회에 오신 여러분 환영합니다! 🎉\n",">\n","> 아래 Tutorial에서는 Baseline code가 어떻게 구성되어 있는지 살펴보겠습니다.\n","\n","# Contents\n","\n","1.   Config 구성\n","2.   Model\n","3.   Lightning Module\n","4.   Train\n","5.   Test, Predict\n","6.   Tips\n","\n","---\n","⚠️ 주의사항\n","```\n","Tutorial 파일은 Baseline code의 이해를 돕기 위한 예제로,\n","Google Colab 환경을 기준으로 작성되어 있습니다.\n","VSCode를 이용한 로컬 환경에서 실행할 경우 결로 수정 등이 필요할 수 있습니다.\n","```\n","---\n"],"metadata":{"id":"5VqAPknIoVE6"}},{"cell_type":"markdown","source":["# 0. Baseline code 다운로드 및 Dependency 설치\n","\n","Colab 환경에서 Baseline code를 실행하기 위한 설정입니다."],"metadata":{"id":"6sxjE00Psk4M"}},{"cell_type":"code","source":["#@markdown ### Colab 노트북 Dependency 설치하기\n","#@markdown - Colab 노트북을 사용하는데 필요한 Packages를 설치합니다.\n","#@markdown ---\n","\n","from IPython.display import clear_output\n","import ipywidgets as widgets\n","\n","def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)\n","C_default = \"\\033[0;39m\"\n","C_yellow = \"\\033[1;93m\"\n","\n","print(C_yellow + \"Install...\" + C_default)\n","# START\n","\n","!pip install gdown==v4.6.3 pathlib==1.0.1\n","\n","# END CODE\n","clear_output()\n","inf('\\u2714 Done', 'success', '50px')"],"metadata":{"cellView":"form","id":"7GQ0Y9Vqt0Dx","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["f8e60ab5271149cf83af910df003189e","f09330497ca94e2086adbfe9dbb8c254","f301fd2044954fc99b824f4a841c7e2a"]},"executionInfo":{"status":"ok","timestamp":1709024899858,"user_tz":-540,"elapsed":8558,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"d58edeb7-6c75-41f2-983e-6d8bf32c7290"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Button(button_style='success', description='✔ Done', disabled=True, layout=Layout(min_width='50px'), style=But…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8e60ab5271149cf83af910df003189e"}},"metadata":{}}]},{"cell_type":"code","source":["#@markdown ### Baseline code 다운로드 (Google Drive)\n","\n","#@markdown 미리 준비된 다음 파일을 설정한 경로에 다운로드 받습니다.\n","#@markdown - Baseline code\n","#@markdown - Test dataset\n","#@markdown - Model checkpoint\n","#@markdown ---\n","\n","print(C_yellow + \"Download...\" + C_default)\n","# START\n","\n","from pathlib import Path\n","\n","Baseline_URL = \"https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000293/data/code.tar.gz\" #@param {type:\"string\"}\n","Library_URL = \"https://drive.google.com/drive/folders/1TYvjiTivRJcIrLytshcEaaooLie9s4pU?usp=sharing\" #@param {type:\"string\"}\n","Dataset_URL = \"https://drive.google.com/drive/folders/1FBEafD3Zua86kb5TodVUgAk5SgRuIUJ4?usp=sharing\" #@param {type:\"string\"}\n","Download_Path = Path('/content')\n","\n","Baseline_Path = Download_Path / \"baseline_code\"\n","Library_Path = Baseline_Path / \"lib\"\n","Dataset_Path = Download_Path / \"dataset\"\n","\n","!mkdir -p {Download_Path}\n","!curl -o {Download_Path}/code.tar.gz {Baseline_URL}\n","!gdown {Library_URL} -O {Library_Path} --folder\n","!gdown {Dataset_URL} -O {Dataset_Path} --folder\n","\n","!mkdir -p /data\n","!ln -s /content/dataset /data/datasets\n","\n","print(C_yellow + \"Install...\" + C_default)\n","!tar xvfz {Download_Path}/code.tar.gz\n","!pip install -r {Library_Path}/requirements.txt\n","!cp -rf {Library_Path}/vgg16.py {Baseline_Path}/ocr/models/encoder/\n","\n","%cd {Baseline_Path}\n","\n","# END CODE\n","clear_output()\n","inf('\\u2714 Done', 'success', '50px')"],"metadata":{"id":"9UhwsQXmuEJx","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["12776edbe7cc4f088574e8be133dd831","87c9f3c8b7254aeead1722fbf430d238","066a11b6ba974aaca5799b28fa10d94c"]},"outputId":"1f86754b-c47b-4207-ead6-d30850e37d9c","cellView":"form","executionInfo":{"status":"ok","timestamp":1709024928692,"user_tz":-540,"elapsed":28838,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Button(button_style='success', description='✔ Done', disabled=True, layout=Layout(min_width='50px'), style=But…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12776edbe7cc4f088574e8be133dd831"}},"metadata":{}}]},{"cell_type":"code","source":["#@markdown ### GPU 사용 설정\n","\n","#@markdown - Colab 환경에서 유효한 Device를 확인합니다.\n","#@markdown ---\n","\n","import torch\n","\n","def to_device(data, device):\n","    if isinstance(data, (list, tuple)):\n","        return [to_device(x, device) for x in data]\n","    elif isinstance(data, dict):\n","        return {k: to_device(v, device) for k, v in data.items()}\n","    elif isinstance(data, torch.Tensor):\n","        return data.to(device)\n","    else:\n","        return data\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","print(f'Use [{device}]')"],"metadata":{"cellView":"form","id":"BEvjbfj7Lgxe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709024930810,"user_tz":-540,"elapsed":2131,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"f26fd3e9-7a2d-4c87-8607-65e32a58dd83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Use [cuda]\n"]}]},{"cell_type":"markdown","source":["# 1. Config 구성\n","\n","Config 구성은 Hydra 기반으로 구성되어 있으며 아래와 같은 Folder structure를 가집니다.\n","\n","```\n","└─── configs\n","    ├── preset\n","    │   ├── example.yaml\n","    │   ├── base.yaml\n","    │   ├── datasets\n","    │   │   └── db.yaml\n","    │   ├── lightning_modules\n","    │   │   └── base.yaml\n","    │   ├── metrics\n","    │   │   └── cleval.yaml\n","    │   └── models\n","    │       ├── decoder\n","    │       │   └── unet.yaml\n","    │       ├── encoder\n","    │       │   └── timm_backbone.yaml\n","    │       ├── head\n","    │       │   └── db_head.yaml\n","    │       ├── loss\n","    │       │   └── db_loss.yaml\n","    │       ├── postprocess\n","    │       │   └── base.yaml\n","    │       └── model_example.yaml\n","    ├── train.yaml\n","    ├── test.yaml\n","    └── predict.yaml\n"," ```"],"metadata":{"id":"H11g2Tq-pCJS"}},{"cell_type":"markdown","source":["## 1.1 Dataset config\n","\n","- configs/preset/datasets/db.yaml\n","\n","> Dataset config에 대해서 알아보자.\n","\n","```yaml\n","# @package _global_\n","\n","dataset_base_path: \"/data/datasets/\"   # Change your path\n","\n","datasets:\n","  train_dataset:\n","    _target_: ${dataset_path}.OCRDataset\n","    image_path: ${dataset_base_path}images/train\n","    annotation_path: ${dataset_base_path}jsons/train.json\n","    transform: ${transforms.train_transform}\n","  val_dataset:\n","    _target_: ${dataset_path}.OCRDataset\n","    image_path: ${dataset_base_path}images/val\n","    annotation_path: ${dataset_base_path}jsons/val.json\n","    transform: ${transforms.val_transform}\n","  test_dataset:\n","    _target_: ${dataset_path}.OCRDataset\n","    image_path: ${dataset_base_path}images/val\n","    annotation_path: ${dataset_base_path}jsons/val.json\n","    transform: ${transforms.test_transform}\n","  predict_dataset:\n","    _target_: ${dataset_path}.OCRDataset\n","    image_path: ${dataset_base_path}images/test\n","    annotation_path: null\n","    transform: ${transforms.test_transform}\n","\n","transforms:\n","  train_transform:\n","    _target_: ${dataset_path}.DBTransforms\n","    transforms:\n","      - _target_: albumentations.LongestMaxSize\n","        max_size: 640\n","        p: 1.0\n","      - _target_: albumentations.PadIfNeeded\n","        min_width: 640\n","        min_height: 640\n","        border_mode: 0\n","        p: 1.0\n","      - _target_: albumentations.HorizontalFlip\n","        p: 0.5\n","      - _target_: albumentations.Normalize\n","        mean: [0.485, 0.456, 0.406]\n","        std: [0.229, 0.224, 0.225]\n","    keypoint_params:\n","      _target_: albumentations.KeypointParams\n","      format: 'xy'\n","      remove_invisible: True\n","```"],"metadata":{"id":"HZDgbkGLp084"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"36Nh0KVFoQPF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709024934094,"user_tz":-540,"elapsed":3286,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"75b3cca7-a215-4414-9cfa-ee0ba84429e5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['image', 'image_filename', 'shape', 'polygons', 'inverse_matrix'])"]},"metadata":{},"execution_count":4}],"source":["import omegaconf\n","from pathlib import Path\n","from hydra.utils import instantiate\n","import sys\n","from hydra import compose, initialize\n","\n","sys.path.append(str(Baseline_Path))\n","\n","config_path = 'configs/'\n","\n","with initialize(version_base=None, config_path=str(config_path), job_name=\"test_app\"):\n","    cfg = compose(config_name=\"train\", overrides=[\"preset=example\"])\n","\n","train_dataset = instantiate(cfg.datasets.train_dataset)\n","\n","data = train_dataset[0]\n","data.keys()"]},{"cell_type":"markdown","source":["```yaml\n","# @package _global_\n","\n","dataloaders:\n","  train_dataloader:\n","    batch_size: 4\n","    shuffle: True\n","    num_workers: 2\n","  val_dataloader:\n","    batch_size: 4\n","    shuffle: False\n","    num_workers: 2\n","  test_dataloader:\n","    batch_size: 4\n","    shuffle: False\n","    num_workers: 2\n","  predict_dataloader:\n","    batch_size: 1\n","    shuffle: False\n","    num_workers: 2\n","\n","collate_fn:\n","  _target_: ${dataset_path}.DBCollateFN\n","  shrink_ratio: 0.4\n","  thresh_min: 0.3\n","  thresh_max: 0.7\n","```"],"metadata":{"id":"u_ADx1sNkHD_"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","collate_fn = instantiate(cfg.collate_fn)\n","collate_fn.inference_mode = False\n","data_loader = DataLoader(train_dataset, collate_fn=collate_fn, **cfg.dataloaders.train_dataloader)\n","for i, data_loaded in enumerate(data_loader):\n","    if i == 1:\n","        break\n","    print(data_loaded.keys())\n","    print(f\"image size: {data_loaded['images'].shape}\")\n","\n","data_loaded = to_device(data_loaded, device)"],"metadata":{"id":"2oznAglYkX1g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709024935363,"user_tz":-540,"elapsed":1272,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"99e1a1f0-5d3d-4be8-b58f-5cdb14c404b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["odict_keys(['images', 'image_filename', 'inverse_matrix', 'polygons', 'prob_maps', 'thresh_maps'])\n","image size: torch.Size([4, 3, 640, 640])\n"]}]},{"cell_type":"markdown","source":["## 1.2 Encoder config\n","\n","- configs/preset/models/encoder/timm_backbone.yaml\n","\n","Encoder - Decoder - Head 구조 중 Encoder config에 대해서 알아보자."],"metadata":{"id":"QNfRu8tdlv1v"}},{"cell_type":"markdown","source":["```yaml\n","# @package _global_\n","\n","models:\n","  encoder:\n","    _target_: ${encoder_path}.TimmBackbone\n","    model_name: 'resnet18'\n","    select_features: [1, 2, 3, 4]            # Output layer\n","    pretrained: true\n","```"],"metadata":{"id":"kvAzQzTrmgCo"}},{"cell_type":"code","source":["encoder = instantiate(cfg.models.encoder).to(device)\n","encoder_features = encoder(data_loaded[\"images\"])\n","for encoder_feature in encoder_features:\n","    print(encoder_feature.shape)"],"metadata":{"id":"z4nbvMaRlvUL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709024938910,"user_tz":-540,"elapsed":3549,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"328353f2-176b-4df1-c6bd-8abf4a17ddfa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([4, 64, 160, 160])\n","torch.Size([4, 128, 80, 80])\n","torch.Size([4, 256, 40, 40])\n","torch.Size([4, 512, 20, 20])\n"]}]},{"cell_type":"markdown","source":["## 1.3 Decoder config\n","\n","- configs/preset/models/decoder/unet.yaml\n","\n","Encoder - Decoder - Head 구조 중 Decoder config에 대해서 알아보자."],"metadata":{"id":"7cKE_ScDoa8U"}},{"cell_type":"markdown","source":["```yaml\n","# @package _global_\n","\n","models:\n","  decoder:\n","    _target_: ${decoder_path}.UNet\n","    in_channels: [64, 128, 256, 512]  # Input layer channel\n","    strides: [4, 8, 16, 32]           # Input layer scale\n","    inner_channels: 256               # Hidden layer channel\n","    output_channels: 64               # output layer channel\n","    bias: False\n","```"],"metadata":{"id":"5S4KnhUcolpy"}},{"cell_type":"code","source":["decoder = instantiate(cfg.models.decoder).to(device)\n","decoder_features = decoder(encoder_features)\n","for decoder_feature in decoder_features:\n","    print(decoder_feature.shape)"],"metadata":{"id":"OiADLRQVoqw2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709024939295,"user_tz":-540,"elapsed":387,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"78ac3c38-726a-4968-a370-183505a0b615"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 64, 160, 160])\n","torch.Size([4, 64, 160, 160])\n","torch.Size([4, 64, 160, 160])\n","torch.Size([4, 64, 160, 160])\n"]}]},{"cell_type":"markdown","source":["## 1.4 Head config\n","\n","- configs/preset/models/head/db_head.yaml\n","\n","Encoder - Decoder - Head 구조 중 Head config에 대해서 알아보자."],"metadata":{"id":"n3Fvcj_Ro6kA"}},{"cell_type":"markdown","source":["```yaml\n","# @package _global_\n","\n","# https://arxiv.org/pdf/1911.08947.pdf 참조\n","\n","models:\n","  head:\n","    _target_: ${head_path}.DBHead\n","    in_channels: 256                 # Input layer channel\n","    upscale: 4                       # Output layer scale factor\n","    k: 50                            # The amplifying factor\n","    bias: False                      # Use bias or not in LayerNorm\n","    smooth: False                    # Use smooth or not in Upsample\n","    postprocess:\n","      thresh: 0.3                    # Binarization threshold\n","      box_thresh: 0.7                # Detection Box threshold\n","      max_candidates: 300            # Limit the number of detection boxes\n","      use_polygon: False             # Detection Box Type (QUAD or POLY)\n","```"],"metadata":{"id":"NRMk0VjcpCd0"}},{"cell_type":"code","source":["head = instantiate(cfg.models.head).to(device)\n","with torch.no_grad():\n","  head_output = head(decoder_features)\n","  for k, v in head_output.items():\n","      print(f'{k}: {v.shape}')"],"metadata":{"id":"sxyu9ce5pFUC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709024939617,"user_tz":-540,"elapsed":324,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"dace3301-bfd3-4ebc-b6c7-a39d53772b8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["prob_maps: torch.Size([4, 1, 640, 640])\n","thresh_maps: torch.Size([4, 1, 640, 640])\n","binary_maps: torch.Size([4, 1, 640, 640])\n"]}]},{"cell_type":"markdown","source":["## 1.5 Loss config\n","\n","- configs/preset/models/loss/db_loss.yaml\n","\n","Loss config에 대해서 알아보자."],"metadata":{"id":"_F1F1u5VpaQw"}},{"cell_type":"markdown","source":["```yaml\n","# @package _global_\n","\n","# https://arxiv.org/pdf/1911.08947.pdf 참조\n","\n","models:\n","  loss:\n","    _target_: ${loss_path}.DBLoss\n","    negative_ratio: 3.0\n","    eps: 1e-6\n","    prob_map_loss_weight: 5.0\n","    thresh_map_loss_weight: 10.0\n","    binary_map_loss_weight: 1.0\n","```"],"metadata":{"id":"mrEHcrv7ph2Y"}},{"cell_type":"code","source":["loss_fn = instantiate(cfg.models.loss).to(device)\n","loss, loss_dict = loss_fn(head_output, **data_loaded)\n","print(loss)\n","for k, v in loss_dict.items():\n","    print(f'{k}: {v}')"],"metadata":{"id":"BfvzJVtEpZ4y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709024940365,"user_tz":-540,"elapsed":750,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"faebfcd6-e0ef-4225-8c90-149260dd0ce9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(30.8433, device='cuda:0')\n","loss_prob: 5.339261531829834\n","loss_thresh: 0.322316437959671\n","loss_binary: 0.9238760471343994\n"]}]},{"cell_type":"markdown","source":["## 1.6 Post Process\n","\n","Model output을 후처리하면 어떠한 output이 나오는지 알아보자."],"metadata":{"id":"RCucSOrUqOMK"}},{"cell_type":"code","source":["boxes_batch, boxes_score = head.get_polygons_from_maps(data_loaded, head_output)"],"metadata":{"id":"xbIWP5Z0qDke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["boxes_batch"],"metadata":{"id":"Nt8YqdOzqP26","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709024942053,"user_tz":-540,"elapsed":5,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"48beea4d-4245-4386-bf37-6476fa6df02c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[[[840, 1264], [866, 1264], [866, 1280], [840, 1280]],\n","  [[1102, 1264], [1114, 1264], [1114, 1274], [1102, 1274]],\n","  [[978, 1260], [988, 1270], [979, 1279], [969, 1269]],\n","  [[969, 1261], [979, 1271], [970, 1280], [960, 1270]],\n","  [[952, 1254], [972, 1264], [964, 1280], [944, 1270]],\n","  [[946, 1258], [956, 1268], [947, 1277], [937, 1267]],\n","  [[920, 1260], [931, 1271], [922, 1280], [911, 1269]],\n","  [[904, 1270], [914, 1260], [922, 1268], [912, 1278]],\n","  [[904, 1260], [915, 1271], [906, 1280], [895, 1269]],\n","  [[895, 1260], [907, 1268], [897, 1282], [885, 1274]],\n","  [[888, 1260], [898, 1270], [889, 1279], [879, 1269]],\n","  [[880, 1260], [890, 1270], [881, 1279], [871, 1269]],\n","  [[858, 1262], [882, 1262], [882, 1278], [858, 1278]],\n","  [[794, 1262], [818, 1262], [818, 1280], [794, 1280]],\n","  [[792, 1260], [803, 1271], [794, 1280], [783, 1269]],\n","  [[600, 1260], [611, 1271], [602, 1280], [591, 1269]],\n","  [[592, 1260], [603, 1271], [594, 1280], [583, 1269]],\n","  [[356, 1264], [366, 1264], [366, 1276], [356, 1276]],\n","  [[236, 1262], [260, 1262], [260, 1280], [236, 1280]]],\n"," [],\n"," [],\n"," [[[1110, 1264], [1120, 1264], [1120, 1276], [1110, 1276]],\n","  [[1102, 1264], [1114, 1264], [1114, 1274], [1102, 1274]],\n","  [[934, 1268], [958, 1256], [966, 1272], [942, 1284]],\n","  [[857, 1259], [868, 1270], [859, 1279], [848, 1268]],\n","  [[834, 1262], [858, 1262], [858, 1280], [834, 1280]],\n","  [[720, 1270], [730, 1260], [738, 1268], [728, 1278]],\n","  [[720, 1260], [731, 1271], [722, 1280], [711, 1269]],\n","  [[704, 1270], [714, 1260], [722, 1268], [712, 1278]],\n","  [[572, 1264], [586, 1264], [586, 1278], [572, 1278]],\n","  [[564, 1264], [578, 1264], [578, 1278], [564, 1278]],\n","  [[561, 1259], [572, 1270], [563, 1279], [552, 1268]],\n","  [[538, 1260], [550, 1272], [540, 1282], [528, 1270]],\n","  [[530, 1260], [540, 1270], [530, 1280], [520, 1270]],\n","  [[522, 1260], [532, 1270], [522, 1280], [512, 1270]],\n","  [[514, 1260], [524, 1270], [514, 1280], [504, 1270]],\n","  [[490, 1260], [500, 1270], [490, 1280], [480, 1270]],\n","  [[481, 1259], [492, 1270], [483, 1279], [472, 1268]],\n","  [[462, 1252], [480, 1270], [464, 1286], [446, 1268]],\n","  [[450, 1260], [460, 1270], [450, 1280], [440, 1270]],\n","  [[386, 1260], [396, 1270], [386, 1280], [376, 1270]],\n","  [[370, 1260], [380, 1270], [370, 1280], [360, 1270]],\n","  [[346, 1260], [356, 1270], [346, 1280], [336, 1270]],\n","  [[230, 1260], [318, 1260], [318, 1282], [230, 1282]],\n","  [[152, 1260], [192, 1260], [192, 1282], [152, 1282]],\n","  [[-2, 1264], [8, 1264], [8, 1276], [-2, 1276]],\n","  [[677, 1261], [711, 1255], [715, 1279], [681, 1285]]]]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["boxes_score"],"metadata":{"id":"wF0Ov7ggqTdI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709024942054,"user_tz":-540,"elapsed":5,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"3f35a4e8-04e1-4eb1-9591-9961cd3a8a62"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0.45524825815111397,\n","  0.6496775825893565,\n","  0.5841146866839967,\n","  0.4793402031799288,\n","  0.5766175248995856,\n","  0.6665217394107266,\n","  0.4715702997577451,\n","  0.5163349043448558,\n","  0.4439139825259417,\n","  0.47516152241761345,\n","  0.5143326961386169,\n","  0.5864490897302845,\n","  0.5763864548336844,\n","  0.49151017881787856,\n","  0.5251596476092263,\n","  0.45600231226953014,\n","  0.46975141108288715,\n","  0.5906513050547801,\n","  0.523324312770927],\n"," [],\n"," [],\n"," [0.7143274578265846,\n","  0.6290705460545724,\n","  0.47780686336541706,\n","  0.41646984848656843,\n","  0.40017052990447993,\n","  0.5309884444706969,\n","  0.5799044613329236,\n","  0.5308339078910649,\n","  0.4260425598886286,\n","  0.4338377337550628,\n","  0.4682258974734892,\n","  0.45017589119242984,\n","  0.48015375061968374,\n","  0.4544194877299015,\n","  0.4410321581177414,\n","  0.4702468290552497,\n","  0.4168512431298785,\n","  0.46145283357291195,\n","  0.48142035766039043,\n","  0.46864132285350935,\n","  0.46602760212146677,\n","  0.4363943788292818,\n","  0.41702349438673997,\n","  0.44712691594112486,\n","  0.513532528722135,\n","  0.449682647100014]]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["## 1.7 Model config\n","\n","- configs/preset/models/model_example.yaml\n","\n","Encoder - Decoder - Head - Loss를 합친 Model config에 대해서 알아보자."],"metadata":{"id":"_ZnxUXKoqgyB"}},{"cell_type":"markdown","source":["```yaml\n","# @package _global_\n","\n","defaults:\n","  - /preset/models/decoder/unet\n","  - /preset/models/encoder/timm_backbone\n","  - /preset/models/head/db_head\n","  - /preset/models/loss/db_loss\n","  - _self_\n","\n","models:\n","  optimizer:\n","    _target_: torch.optim.Adam\n","    lr: 0.001\n","    weight_decay: 0.0001\n","  scheduler:\n","    _target_: torch.optim.lr_scheduler.StepLR\n","    step_size: 100\n","    gamma: 0.1\n","```"],"metadata":{"id":"VbKucQL_qx0X"}},{"cell_type":"markdown","source":["## 1.8 Example config\n","\n","- configs/preset/example.yaml\n","\n","Dataset, Model, Base setting config를 합친 Example config"],"metadata":{"id":"n5Pc3HwIrEfc"}},{"cell_type":"markdown","source":["```yaml\n","# @package _global_\n","\n","defaults:\n","  - base\n","  - /preset/datasets/db\n","  - /preset/models/model_example\n","  - /preset/lightning_modules/base\n","  - _self_\n","```"],"metadata":{"id":"X64UDatKrF4K"}},{"cell_type":"markdown","source":["## 1.9 Train config\n","\n","- configs/train.yaml\n","\n","Train 관련 config에 대해서 알아보자."],"metadata":{"id":"AOPVmyDQrND3"}},{"cell_type":"markdown","source":["```yaml\n","defaults:\n","  - _self_\n","  - preset:\n","  - override hydra/hydra_logging: disabled\n","  - override hydra/job_logging: disabled\n","\n","seed: 42\n","exp_name: \"ocr_training\"\n","project_name: \"OCRProject\"\n","\n","wandb: False\n","exp_version: \"v1.0\"\n","\n","resume: null  # \"checkpoints/sehwan_20240118_144938/epoch=5-step=4908.ckpt\"\n","\n","trainer:\n","  max_epochs: 10\n","  num_sanity_val_steps: 1\n","  log_every_n_steps: 50\n","  check_val_every_n_epoch: 1\n","  deterministic: True\n","```"],"metadata":{"id":"lvS5yebQrVch"}},{"cell_type":"markdown","source":["## 1.10 Test config\n","\n","- configs/test.yaml\n","\n","Test 관련 config에 대해서 알아보자."],"metadata":{"id":"VJSXft17rn_P"}},{"cell_type":"markdown","source":["```yaml\n","defaults:\n","  - _self_\n","  - preset:\n","  - override hydra/hydra_logging: disabled\n","  - override hydra/job_logging: disabled\n","\n","seed: 42\n","exp_name: \"ocr_training\"\n","project_name: \"OCRProject\"\n","\n","wandb: False\n","exp_version: \"v1.0\"\n","\n","checkpoint_path: \"checkpoints/sehwan_20240118_144938/epoch=5-step=4908.ckpt\"\n","```"],"metadata":{"id":"23sUSDhHruRC"}},{"cell_type":"markdown","source":["## 1.11 Predict config\n","\n","- configs/predict.yaml\n","\n","Predict 관련 config에 대해서 알아보자."],"metadata":{"id":"9GD-XZXWsBcK"}},{"cell_type":"markdown","source":["```yaml\n","defaults:\n","  - _self_\n","  - preset:\n","  - override hydra/hydra_logging: disabled\n","  - override hydra/job_logging: disabled\n","\n","seed: 42\n","exp_name: \"ocr_training\"\n","\n","checkpoint_path: \"checkpoints/sehwan_20240118_144938/epoch=5-step=4908.ckpt\"\n","minified_json: False\n","```"],"metadata":{"id":"6-soCKdbsFu7"}},{"cell_type":"markdown","source":["# 2. Model\n","\n","Model의 구현체는 어떻게 생겼는지 알아보자.\n","\n","1.   Encoder\n","2.   Decoder\n","3.   Architecture\n","\n"],"metadata":{"id":"Ou37itr0sNxA"}},{"cell_type":"markdown","source":["## 2.1 Encoder"],"metadata":{"id":"muoxgk-8sZyP"}},{"cell_type":"code","source":["import torch.nn as nn\n","import timm\n","\n","\n","class TimmBackbone(nn.Module):\n","    def __init__(self, model_name='resnet18', select_features=[1, 2, 3, 4], pretrained=True):\n","        super(TimmBackbone, self).__init__()\n","        # Timm Backbone 모델을 자유롭게 사용\n","        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n","        # Decoder에 연결하려는 Feature를 선택\n","        self.select_features = select_features\n","\n","    def forward(self, x):\n","        features = self.model(x)\n","        return [features[i] for i in self.select_features]"],"metadata":{"id":"UpBxd8Hlscd1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 Decoder"],"metadata":{"id":"Rrv63B9Asler"}},{"cell_type":"code","source":["from itertools import accumulate\n","import torch.nn as nn\n","\n","\n","class UNet(nn.Module):\n","    def __init__(self,\n","                 in_channels=[64, 128, 256, 512],\n","                 strides=[4, 8, 16, 32],\n","                 inner_channels=256,\n","                 output_channels=64,\n","                 bias=False):\n","        super(UNet, self).__init__()\n","\n","        assert len(strides) == len(in_channels), \"Mismatch in 'strides' and 'in_channels' lengths.\"\n","\n","        # Parameters에 따라 UNet 구조를 동적으로 생성\n","        # Decoder size 계산\n","        upscale_factors = [strides[idx] // strides[idx - 1] for idx in range(1, len(strides))]\n","        outscale_factors = list(accumulate(upscale_factors, lambda x, y: x * y))\n","\n","        self.upsamples = nn.ModuleList()\n","        for upscale in upscale_factors:\n","            self.upsamples.append(nn.Upsample(scale_factor=upscale, mode='nearest'))\n","\n","        self.inners = nn.ModuleList()\n","        for in_channel in in_channels:\n","            self.inners.append(nn.Conv2d(in_channel, inner_channels, kernel_size=1, bias=bias))\n","\n","        self.outers = nn.ModuleList()\n","        for outscale in reversed(outscale_factors):\n","            outer = nn.Sequential(nn.Conv2d(inner_channels, output_channels,\n","                                            kernel_size=3, padding=1, bias=bias),\n","                                  nn.Upsample(scale_factor=outscale, mode='nearest'))\n","            self.outers.append(outer)\n","        self.outers.append(nn.Conv2d(inner_channels, output_channels, kernel_size=3,\n","                                     padding=1, bias=bias))\n","\n","        self.upsamples.apply(self.weights_init)\n","        self.inners.apply(self.weights_init)\n","        self.outers.apply(self.weights_init)\n","\n","    def weights_init(self, m):\n","        classname = m.__class__.__name__\n","        if classname.find('Conv') != -1:\n","            nn.init.kaiming_normal_(m.weight.data)\n","        elif classname.find('BatchNorm') != -1:\n","            m.weight.data.fill_(1.)\n","            m.bias.data.fill_(1e-4)\n","\n","    def forward(self, features):\n","        in_features = [inner(feat) for feat, inner in zip(features, self.inners)]\n","\n","        up_features = []\n","        up = in_features[-1]\n","        for i in range(len(in_features) - 1, 0, -1):\n","            up = self.upsamples[i - 1](up) + in_features[i - 1]\n","            up_features.append(up)\n","\n","        out_features = [self.outers[0](in_features[-1])]\n","        out_features += [outer(feat) for feat, outer in zip(up_features, self.outers[1:])]\n","\n","        return out_features"],"metadata":{"id":"ZMXNqTensumU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3 Architecture"],"metadata":{"id":"HwSmcsErsyR_"}},{"cell_type":"code","source":["import torch.nn as nn\n","from hydra.utils import instantiate\n","from ocr.models.encoder import get_encoder_by_cfg\n","from ocr.models.decoder import get_decoder_by_cfg\n","from ocr.models.head import get_head_by_cfg\n","from ocr.models.loss import get_loss_by_cfg\n","\n","\n","class OCRModel(nn.Module):\n","    def __init__(self, cfg):\n","        super(OCRModel, self).__init__()\n","        self.cfg = cfg\n","\n","        # 각 모듈 instantiate\n","        self.encoder = get_encoder_by_cfg(cfg.encoder)\n","        self.decoder = get_decoder_by_cfg(cfg.decoder)\n","        self.head = get_head_by_cfg(cfg.head)\n","        self.loss = get_loss_by_cfg(cfg.loss)\n","\n","    def forward(self, images, return_loss=True, **kwargs):\n","        encoded_features = self.encoder(images)\n","        decoded_features = self.decoder(encoded_features)\n","        pred = self.head(decoded_features, return_loss)\n","\n","        # Loss 계산\n","        if return_loss:\n","            loss, loss_dict = self.loss(pred, **kwargs)\n","            pred.update(loss=loss, loss_dict=loss_dict)\n","\n","        return pred\n","\n","    def get_optimizers(self):\n","        optimizer_config = self.cfg.optimizer\n","        optimizer = instantiate(optimizer_config, params=self.parameters())\n","\n","        if 'scheduler' in self.cfg:\n","            scheduler_config = self.cfg.scheduler\n","            scheduler = instantiate(scheduler_config, optimizer=optimizer)\n","            return [optimizer], [scheduler]\n","        return optimizer\n","\n","    def get_polygons_from_maps(self, gt, pred):\n","        return self.head.get_polygons_from_maps(gt, pred)"],"metadata":{"id":"6lm4hh-Ss0S1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Lightning Module\n","\n","Lightning의 Module에 대해서 알아보자."],"metadata":{"id":"Hz5k-9W9s_ke"}},{"cell_type":"markdown","source":["## 3.1 Lightning Trainer"],"metadata":{"id":"n6s6GJ4TtHUX"}},{"cell_type":"code","source":["import numpy as np\n","import json\n","from datetime import datetime\n","from pathlib import Path\n","import lightning.pytorch as pl\n","from tqdm import tqdm\n","from collections import defaultdict\n","from collections import OrderedDict\n","from torch.utils.data import DataLoader\n","from hydra.utils import instantiate\n","from ocr.metrics import CLEvalMetric\n","\n","\n","class OCRPLModule(pl.LightningModule):\n","    def __init__(self, model, dataset, config):\n","        super(OCRPLModule, self).__init__()\n","        self.model = model\n","        self.dataset = dataset\n","        self.metric = CLEvalMetric()\n","        self.config = config\n","\n","        self.validation_step_outputs = OrderedDict()\n","        self.test_step_outputs = OrderedDict()\n","        self.predict_step_outputs = OrderedDict()\n","\n","    def forward(self, x):\n","        return self.model(return_loss=False, **x)\n","\n","    def training_step(self, batch, batch_idx):\n","        pred = self.model(**batch)\n","        self.log('train/loss', pred['loss'], batch_size=len(batch))\n","        for key, value in pred['loss_dict'].items():\n","            self.log(f'train/{key}', value, batch_size=len(batch))\n","        return pred\n","\n","    def validation_step(self, batch, batch_idx):\n","        pred = self.model(**batch)\n","        self.log('val/loss', pred['loss'], batch_size=len(batch))\n","        for key, value in pred['loss_dict'].items():\n","            self.log(f'val/{key}', value, batch_size=len(batch))\n","\n","        boxes_batch, _ = self.model.get_polygons_from_maps(batch, pred)\n","        for idx, boxes in enumerate(boxes_batch):\n","            self.validation_step_outputs[batch['image_filename'][idx]] = boxes\n","        return pred\n","\n","    def on_validation_epoch_end(self):\n","        cleval_metrics = defaultdict(list)\n","\n","        for gt_filename, gt_words in tqdm(self.dataset['val'].anns.items(), desc=\"Evaluation\"):\n","            if gt_filename not in self.validation_step_outputs:\n","                # TODO: Check if this is on_sanity?\n","                cleval_metrics['recall'].append(np.array(0., dtype=np.float32))\n","                cleval_metrics['precision'].append(np.array(0., dtype=np.float32))\n","                cleval_metrics['hmean'].append(np.array(0., dtype=np.float32))\n","                continue\n","\n","            pred = self.validation_step_outputs[gt_filename]\n","            det_quads = [[point for coord in polygons for point in coord]\n","                         for polygons in pred]\n","            gt_quads = [item.squeeze().reshape(-1) for item in gt_words]\n","\n","            self.metric(det_quads, gt_quads)\n","            cleval = self.metric.compute()\n","            cleval_metrics['recall'].append(cleval['det_r'].cpu().numpy())\n","            cleval_metrics['precision'].append(cleval['det_p'].cpu().numpy())\n","            cleval_metrics['hmean'].append(cleval['det_h'].cpu().numpy())\n","            self.metric.reset()\n","\n","        recall = np.mean(cleval_metrics['recall'])\n","        precision = np.mean(cleval_metrics['precision'])\n","        hmean = np.mean(cleval_metrics['hmean'])\n","\n","        self.log('val/recall', recall, on_epoch=True, prog_bar=True)\n","        self.log('val/precision', precision, on_epoch=True, prog_bar=True)\n","        self.log('val/hmean', hmean, on_epoch=True, prog_bar=True)\n","\n","        self.validation_step_outputs.clear()\n","\n","    def test_step(self, batch):\n","        pred = self.model(return_loss=False, **batch)\n","\n","        boxes_batch, _ = self.model.get_polygons_from_maps(batch, pred)\n","        for idx, boxes in enumerate(boxes_batch):\n","            self.test_step_outputs[batch['image_filename'][idx]] = boxes\n","        return pred\n","\n","    def on_test_epoch_end(self):\n","        cleval_metrics = defaultdict(list)\n","\n","        for gt_filename, gt_words in tqdm(self.dataset['test'].anns.items(), desc=\"Evaluation\"):\n","            pred = self.test_step_outputs[gt_filename]\n","            det_quads = [[point for coord in polygons for point in coord]\n","                         for polygons in pred]\n","            gt_quads = [item.squeeze().reshape(-1) for item in gt_words]\n","\n","            self.metric(det_quads, gt_quads)\n","            cleval = self.metric.compute()\n","            cleval_metrics['recall'].append(cleval['det_r'].cpu().numpy())\n","            cleval_metrics['precision'].append(cleval['det_p'].cpu().numpy())\n","            cleval_metrics['hmean'].append(cleval['det_h'].cpu().numpy())\n","            self.metric.reset()\n","\n","        recall = np.mean(cleval_metrics['recall'])\n","        precision = np.mean(cleval_metrics['precision'])\n","        hmean = np.mean(cleval_metrics['hmean'])\n","\n","        self.log('test/recall', recall, on_epoch=True, prog_bar=True)\n","        self.log('test/precision', precision, on_epoch=True, prog_bar=True)\n","        self.log('test/hmean', hmean, on_epoch=True, prog_bar=True)\n","\n","        self.test_step_outputs.clear()\n","\n","    def predict_step(self, batch):\n","        pred = self.model(return_loss=False, **batch)\n","        boxes_batch, _ = self.model.get_polygons_from_maps(batch, pred)\n","\n","        for idx, boxes in enumerate(boxes_batch):\n","            self.predict_step_outputs[batch['image_filename'][idx]] = boxes\n","        return pred\n","\n","    def on_predict_epoch_end(self):\n","        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        submission_file = Path(f\"{self.config.submission_dir}\") / f\"{timestamp}.json\"\n","        submission_file.parent.mkdir(parents=True, exist_ok=True)\n","\n","        submission = OrderedDict(images=OrderedDict())\n","        for filename, pred_boxes in self.predict_step_outputs.items():\n","            # Separate box\n","            boxes = OrderedDict()\n","            for idx, box in enumerate(pred_boxes):\n","                boxes[f'{idx + 1:04}'] = OrderedDict(points=box)\n","\n","            # Append box\n","            submission['images'][filename] = OrderedDict(words=boxes)\n","\n","        # Export submission\n","        with submission_file.open(\"w\") as fp:\n","            if self.config.minified_json:\n","                json.dump(submission, fp, indent=None, separators=(',', ':'))\n","            else:\n","                json.dump(submission, fp, indent=4)\n","\n","        self.predict_step_outputs.clear()\n","\n","    def configure_optimizers(self):\n","        return self.model.get_optimizers()"],"metadata":{"id":"XaRmUtNwtLWR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2 Lightning Data Module"],"metadata":{"id":"1dt6O5Y6tb2j"}},{"cell_type":"code","source":["class OCRDataPLModule(pl.LightningDataModule):\n","    def __init__(self, dataset, config):\n","        super(OCRDataPLModule, self).__init__()\n","        self.dataset = dataset\n","        self.config = config\n","        self.collate_fn = instantiate(self.config.collate_fn)\n","\n","    def train_dataloader(self):\n","        train_loader_config = self.config.dataloaders.train_dataloader\n","        self.collate_fn.inference_mode = False\n","        return DataLoader(self.dataset['train'], collate_fn=self.collate_fn, **train_loader_config)\n","\n","    def val_dataloader(self):\n","        val_loader_config = self.config.dataloaders.val_dataloader\n","        self.collate_fn.inference_mode = False\n","        return DataLoader(self.dataset['val'], collate_fn=self.collate_fn, **val_loader_config)\n","\n","    def test_dataloader(self):\n","        test_loader_config = self.config.dataloaders.test_dataloader\n","        self.collate_fn.inference_mode = False\n","        return DataLoader(self.dataset['test'], collate_fn=self.collate_fn, **test_loader_config)\n","\n","    def predict_dataloader(self):\n","        predict_loader_config = self.config.dataloaders.predict_dataloader\n","        self.collate_fn.inference_mode = True\n","        return DataLoader(self.dataset['predict'], collate_fn=self.collate_fn,\n","                          **predict_loader_config)"],"metadata":{"id":"0PbpSe4ttiY3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Train\n","\n","명령어를 통해 어떻게 학습을 시킬 수 있는지, 학습 실행 코드는 어떻게 이루어져 있는지 알아보자"],"metadata":{"id":"qfl2TLTftmKD"}},{"cell_type":"code","source":["!python runners/train.py preset=example"],"metadata":{"id":"SAjLN_8ntt3z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709025000257,"user_tz":-540,"elapsed":56296,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"4d4c54f9-de91-451c-c245-8458b0df9d42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-27 09:09:14.631496: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-27 09:09:14.631547: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-27 09:09:14.632930: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-02-27 09:09:15.746780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:639: Checkpoint directory /content/baseline_code/outputs/ocr_training/checkpoints exists and is not empty.\n","Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Sanity Checking DataLoader 0: 100% 1/1 [00:01<00:00,  1.38s/it]\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n","Evaluation:  25% 1/4 [00:00<00:01,  2.01it/s]\u001b[A\n","Evaluation: 100% 4/4 [00:00<00:00,  5.64it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","Epoch 0: 100% 1/1 [00:01<00:00,  1.35s/it, v_num=v1.0]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/1 [00:00<?, ?it/s]        \u001b[A\n","Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0: 100% 1/1 [00:00<00:00,  3.11it/s]\u001b[A\n","\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Evaluation:  25% 1/4 [00:00<00:01,  1.91it/s]\u001b[A\u001b[A\n","\n","Evaluation:  50% 2/4 [00:00<00:00,  3.58it/s]\u001b[A\u001b[A\n","\n","Evaluation: 100% 4/4 [00:00<00:00,  5.07it/s]\n","\n","Epoch 1:   0% 0/1 [00:00<?, ?it/s, v_num=v1.0, val/recall=0.000, val/precision=0.000, val/hmean=0.000]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Epoch 1: 100% 1/1 [00:01<00:00,  1.18s/it, v_num=v1.0, val/recall=0.000, val/precision=0.000, val/hmean=0.000]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/1 [00:00<?, ?it/s]        \u001b[A\n","Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0: 100% 1/1 [00:00<00:00,  2.03it/s]\u001b[A\n","\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Evaluation:  50% 2/4 [00:00<00:00, 17.35it/s]\u001b[A\u001b[A\n","\n","Evaluation: 100% 4/4 [00:00<00:00, 14.86it/s]\n","\n","Epoch 2: 100% 1/1 [00:01<00:00,  1.01s/it, v_num=v1.0, val/recall=0.000, val/precision=0.000, val/hmean=0.000]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/1 [00:00<?, ?it/s]        \u001b[A\n","Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0: 100% 1/1 [00:00<00:00,  1.98it/s]\u001b[A\n","\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Evaluation:  50% 2/4 [00:00<00:00, 16.78it/s]\u001b[A\u001b[A\n","\n","Evaluation: 100% 4/4 [00:00<00:00, 14.68it/s]\n","\n","Epoch 3: 100% 1/1 [00:00<00:00,  1.08it/s, v_num=v1.0, val/recall=0.000, val/precision=0.000, val/hmean=0.000]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/1 [00:00<?, ?it/s]        \u001b[A\n","Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0: 100% 1/1 [00:00<00:00,  1.77it/s]\u001b[A\n","\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Evaluation:  50% 2/4 [00:00<00:00,  3.18it/s]\u001b[A\u001b[A\n","\n","Evaluation: 100% 4/4 [00:00<00:00,  5.18it/s]\n","\n","Epoch 4: 100% 1/1 [00:00<00:00,  1.07it/s, v_num=v1.0, val/recall=0.000, val/precision=0.000, val/hmean=0.000]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/1 [00:00<?, ?it/s]        \u001b[A\n","Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0: 100% 1/1 [00:00<00:00,  1.23it/s]\u001b[A\n","\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Evaluation:  50% 2/4 [00:00<00:00, 11.71it/s]\u001b[A\u001b[A\n","\n","Evaluation: 100% 4/4 [00:00<00:00, 10.10it/s]\n","\n","Epoch 5: 100% 1/1 [00:01<00:00,  1.33s/it, v_num=v1.0, val/recall=0.000, val/precision=0.000, val/hmean=0.000]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/1 [00:00<?, ?it/s]        \u001b[A\n","Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0: 100% 1/1 [00:00<00:00,  1.66it/s]\u001b[A\n","\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Evaluation:  50% 2/4 [00:00<00:00, 17.29it/s]\u001b[A\u001b[A\n","\n","Evaluation: 100% 4/4 [00:00<00:00, 15.69it/s]\n","\n","Epoch 6: 100% 1/1 [00:00<00:00,  1.08it/s, v_num=v1.0, val/recall=0.000, val/precision=0.000, val/hmean=0.000]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/1 [00:00<?, ?it/s]        \u001b[A\n","Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0: 100% 1/1 [00:00<00:00,  1.68it/s]\u001b[A\n","\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Evaluation:  50% 2/4 [00:00<00:00, 17.36it/s]\u001b[A\u001b[A\n","\n","Evaluation: 100% 4/4 [00:00<00:00, 15.17it/s]\n","\n","Epoch 7: 100% 1/1 [00:00<00:00,  1.01it/s, v_num=v1.0, val/recall=0.000, val/precision=0.000, val/hmean=0.000]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/1 [00:00<?, ?it/s]        \u001b[A\n","Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0: 100% 1/1 [00:00<00:00,  1.74it/s]\u001b[A\n","\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Evaluation:  50% 2/4 [00:00<00:00, 16.48it/s]\u001b[A\u001b[A\n","\n","Evaluation: 100% 4/4 [00:00<00:00, 14.29it/s]\n","\n","Epoch 8: 100% 1/1 [00:00<00:00,  1.00it/s, v_num=v1.0, val/recall=0.000, val/precision=0.000, val/hmean=0.000]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/1 [00:00<?, ?it/s]        \u001b[A\n","Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0: 100% 1/1 [00:00<00:00,  1.23it/s]\u001b[A\n","\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Evaluation:  50% 2/4 [00:00<00:00,  9.82it/s]\u001b[A\u001b[A\n","\n","Evaluation:  75% 3/4 [00:00<00:00,  8.11it/s]\u001b[A\u001b[A\n","\n","Evaluation: 100% 4/4 [00:00<00:00,  8.70it/s]\n","\n","Epoch 9: 100% 1/1 [00:00<00:00,  1.02it/s, v_num=v1.0, val/recall=0.000, val/precision=0.000, val/hmean=0.000]\n","Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n","Validation:   0% 0/1 [00:00<?, ?it/s]        \u001b[A\n","Validation DataLoader 0:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n","Validation DataLoader 0: 100% 1/1 [00:00<00:00,  2.16it/s]\u001b[A\n","\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Evaluation:  50% 2/4 [00:00<00:00, 15.74it/s]\u001b[A\u001b[A\n","\n","Evaluation: 100% 4/4 [00:00<00:00, 14.13it/s]\n","\n","Epoch 9: 100% 1/1 [00:03<00:00,  3.26s/it, v_num=v1.0, val/recall=0.000, val/precision=0.000, val/hmean=0.000]\n","Testing DataLoader 0: 100% 1/1 [00:00<00:00,  2.07it/s]\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n","Evaluation:  50% 2/4 [00:00<00:00, 15.52it/s]\u001b[A\n","Evaluation: 100% 4/4 [00:00<00:00, 13.64it/s]\n","Testing DataLoader 0: 100% 1/1 [00:00<00:00,  1.27it/s]\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m       test/hmean        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m     test/precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m       test/recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]}]},{"cell_type":"markdown","source":["```yaml\n","defaults:\n","  - _self_\n","  - preset: example\n","  - override hydra/hydra_logging: disabled\n","  - override hydra/job_logging: disabled\n","\n","seed: 42\n","exp_name: \"ocr_training\"\n","project_name: \"OCRProject\"\n","\n","wandb: False\n","exp_version: \"v1.0\"\n","\n","resume: null  # \"checkpoints/sehwan_20240118_144938/epoch=5-step=4908.ckpt\"\n","\n","trainer:\n","  max_epochs: 10\n","  num_sanity_val_steps: 1\n","  log_every_n_steps: 50\n","  check_val_every_n_epoch: 1\n","  deterministic: True\n","```"],"metadata":{"id":"mrdKJCHDwgUK"}},{"cell_type":"markdown","source":["```python\n","@hydra.main(config_path=CONFIG_DIR, config_name='train', version_base='1.2')\n","def train(config):\n","    model_module, data_module = get_pl_modules_by_cfg(config)\n","\n","    trainer = pl.Trainer(\n","        **config.trainer,\n","        logger=logger,\n","        callbacks=callbacks\n","    )\n","\n","    trainer.fit(\n","        model_module,\n","        data_module,\n","        ckpt_path=config.get(\"resume\", None),\n","    )\n","    trainer.test(\n","        model_module,\n","        data_module,\n","    )\n","\n","\n","if __name__ == \"__main__\":\n","    train()\n","```"],"metadata":{"id":"f-83-XYUxVBE"}},{"cell_type":"markdown","source":["# 5. Test, Predict\n","\n","명령어를 통해서 Test, Predict는 어떻게 하는지, Test, Predict 실행 코드는 어떻게 구성되어 있는지 알아보자."],"metadata":{"id":"sSknxhPixhw_"}},{"cell_type":"code","source":["!python runners/test.py preset=example \"checkpoint_path='lib/dbnet-baseline.ckpt'\""],"metadata":{"id":"FUREmcNGxW-F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709025018194,"user_tz":-540,"elapsed":17951,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"d31da6eb-dae1-4bd2-a9af-133145109269"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-02-27 09:10:08.461757: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-27 09:10:08.461833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-27 09:10:08.463237: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-02-27 09:10:09.776315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Testing DataLoader 0: 100% 1/1 [00:00<00:00,  2.16it/s]\n","Evaluation:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n","Evaluation:  25% 1/4 [00:01<00:03,  1.14s/it]\u001b[A\n","Evaluation:  50% 2/4 [00:01<00:01,  1.47it/s]\u001b[A\n","Evaluation:  75% 3/4 [00:02<00:00,  1.62it/s]\u001b[A\n","Evaluation: 100% 4/4 [00:02<00:00,  1.65it/s]\n","Testing DataLoader 0: 100% 1/1 [00:02<00:00,  2.91s/it]\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m       test/hmean        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7507498264312744    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m     test/precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9591268301010132    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m       test/recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6375572681427002    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]}]},{"cell_type":"markdown","source":["```python\n","@hydra.main(config_path=CONFIG_DIR, config_name='test', version_base='1.2')\n","def test(config):\n","    model_module, data_module = get_pl_modules_by_cfg(config)\n","\n","    trainer = pl.Trainer(\n","        logger=logger,\n","    )\n","\n","    trainer.test(\n","        model_module,\n","        data_module,\n","        ckpt_path=config.get(\"checkpoint_path\", None),\n","    )\n","\n","\n","if __name__ == \"__main__\":\n","    test()\n","```"],"metadata":{"id":"S44AMxzizIEs"}},{"cell_type":"code","source":["!python runners/predict.py preset=example \"checkpoint_path='lib/dbnet-baseline.ckpt'\""],"metadata":{"id":"kzsr21Bcza6J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709025028860,"user_tz":-540,"elapsed":10679,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"64be9a86-2b6b-416e-fdb7-281b49b7697c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Predicting DataLoader 0: 100% 4/4 [00:00<00:00,  6.38it/s]\n"]}]},{"cell_type":"markdown","source":["```python\n","@hydra.main(config_path=CONFIG_DIR, config_name='predict', version_base='1.2')\n","def predict(config):\n","    model_module, data_module = get_pl_modules_by_cfg(config)\n","\n","    trainer = pl.Trainer()\n","\n","    pred = trainer.predict(model_module,\n","                           data_module,\n","                           ckpt_path=config.get(\"checkpoint_path\"),\n","                           )\n","\n","\n","if __name__ == \"__main__\":\n","    predict()\n","```"],"metadata":{"id":"xicnuBtNzdHI"}},{"cell_type":"code","source":["#@markdown ### 메모리 해제\n","\n","#@markdown - 미사용 자원을 반환하고 메모리를 확보합니다.\n","#@markdown ---\n","\n","import gc\n","\n","try:\n","  if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","\n","  del encoder\n","  del decoder\n","  del head\n","  del head_output\n","  del loss_fn\n","\n","  gc.collect()\n","\n","  print(\"Clear!\")\n","\n","except:\n","  pass"],"metadata":{"cellView":"form","id":"q6jOsm2kSWua","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709025028861,"user_tz":-540,"elapsed":14,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"adb422f0-07a5-4c42-b48b-32708efb18f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clear!\n"]}]},{"cell_type":"markdown","source":["# 6. Tips\n","\n","1.   Logging\n","2.   Encoder 교체\n","3.   Decoder 교체\n","4.   Head 교체\n","5.   Dataset augmentation\n","6.   후처리 고도화\n","7.   Customize model\n","\n"],"metadata":{"id":"rD92Nl9rzzmJ"}},{"cell_type":"markdown","source":["## 6.1 Logging"],"metadata":{"id":"mPhgNoWe0Buo"}},{"cell_type":"markdown","source":["```bash\n","!python runners/train.py preset=example exp_name=sehwan wandb=True\n","\n","# https://wandb.ai/sehwan-joo_up/OCRProject?workspace=user-sehwan-joo_up\n","```"],"metadata":{"id":"Io5A32UW28fp"}},{"cell_type":"markdown","source":["```bash\n","!python runners/train.py preset=example exp_name=sehwan wandb=False\n","!tensorboard --log_dirs={logging_path} --port={port_number}\n","```"],"metadata":{"id":"s2N0I62r2sYC"}},{"cell_type":"markdown","source":["## 6.2 Encoder 교체"],"metadata":{"id":"mn1Xv77D3D9J"}},{"cell_type":"code","source":["import torch.nn as nn\n","import timm\n","\n","\n","class TimmBackbone(nn.Module):\n","    def __init__(self, model_name='resnet18', select_features=[1, 2, 3, 4], pretrained=True):\n","        super(TimmBackbone, self).__init__()\n","        # Timm Backbone 모델을 자유롭게 사용\n","        self.model = timm.create_model(model_name, pretrained=pretrained, features_only=True)\n","        # Decoder에 연결하려는 Feature를 선택\n","        self.select_features = select_features\n","\n","    def forward(self, x):\n","        features = self.model(x)\n","        return [features[i] for i in self.select_features]"],"metadata":{"id":"RJIQog9j3RJr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["```yaml\n","# @package _global_\n","\n","models:\n","  encoder:\n","    _target_: ${encoder_path}.TimmBackbone\n","    model_name: 'convnext_pico.d1_in1k'   #resnet18\n","    select_features: [1, 2, 3, 4]            # Output layer\n","    pretrained: true\n","```"],"metadata":{"id":"bW84fCwz3W-a"}},{"cell_type":"code","source":["cfg.models.encoder.model_name = 'convnext_pico.d1_in1k'\n","\n","encoder = instantiate(cfg.models.encoder).to(device)\n","encoder_features = encoder(data_loaded[\"images\"])\n","for encoder_feature in encoder_features:\n","    print(encoder_feature.shape)"],"metadata":{"id":"Wi6QsPX93xGg","colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"status":"error","timestamp":1709025029855,"user_tz":-540,"elapsed":1006,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"eed6dc45-aff6-46cf-b093-85d67297acff"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-873685c04d60>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstantiate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mencoder_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mencoder_feature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoder_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/baseline_code/ocr/models/encoder/timm_backbone.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/baseline_code/ocr/models/encoder/timm_backbone.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","source":["for feature in encoder.model(data_loaded[\"images\"]):\n","    print(feature.shape)"],"metadata":{"id":"71kQnD4c4YWR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709025080206,"user_tz":-540,"elapsed":269,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"c6ae3811-54f6-42f3-f902-a285caf7ba30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 64, 160, 160])\n","torch.Size([4, 128, 80, 80])\n","torch.Size([4, 256, 40, 40])\n","torch.Size([4, 512, 20, 20])\n"]}]},{"cell_type":"code","source":["cfg.models.encoder.model_name = 'convnext_pico.d1_in1k'\n","cfg.models.encoder.select_features = [0, 1, 2, 3]\n","\n","encoder = instantiate(cfg.models.encoder).to(device)\n","encoder_features = encoder(data_loaded[\"images\"])\n","for encoder_feature in encoder_features:\n","    print(encoder_feature.shape)"],"metadata":{"id":"oT_5FLAt4ka7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709025080985,"user_tz":-540,"elapsed":326,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"09c85a4d-3550-4f86-abc0-558613f2f01c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 64, 160, 160])\n","torch.Size([4, 128, 80, 80])\n","torch.Size([4, 256, 40, 40])\n","torch.Size([4, 512, 20, 20])\n"]}]},{"cell_type":"markdown","source":["## 6.3 Decoder 교체"],"metadata":{"id":"AzipOwip5HNc"}},{"cell_type":"markdown","source":["```yaml\n","# @package _global_\n","\n","models:\n","  decoder:\n","    _target_: ${decoder_path}.UNet\n","    in_channels: [64, 128, 256, 512]  # [64, 128, 256, 512]\n","    strides: [4, 8, 16, 32]           # [4, 8, 16, 32]\n","    inner_channels: 256               # Hidden layer channel\n","    output_channels: 64               # output layer channel\n","    bias: False\n","```"],"metadata":{"id":"Lwk8HB4v5KDx"}},{"cell_type":"code","source":["cfg.models.decoder.in_channels = [64, 128, 256, 512]\n","cfg.models.decoder.strides = [4, 8, 16, 32]\n","decoder = instantiate(cfg.models.decoder).to(device)\n","decoder_features = decoder(encoder_features)\n","for decoder_feature in decoder_features:\n","    print(decoder_feature.shape)"],"metadata":{"id":"zaAkahgp6IHl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709025083281,"user_tz":-540,"elapsed":296,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"064b32d6-c40a-482b-e611-5e3226829100"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 64, 160, 160])\n","torch.Size([4, 64, 160, 160])\n","torch.Size([4, 64, 160, 160])\n","torch.Size([4, 64, 160, 160])\n"]}]},{"cell_type":"code","source":["cfg.models.decoder.output_channels = 256\n","decoder = instantiate(cfg.models.decoder).to(device)\n","decoder_features = decoder(encoder_features)\n","for decoder_feature in decoder_features:\n","    print(decoder_feature.shape)"],"metadata":{"id":"rVZ1MPJM6Stc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709025085179,"user_tz":-540,"elapsed":374,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"e8494301-9ebd-4580-b912-52d38f7db7fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 256, 160, 160])\n","torch.Size([4, 256, 160, 160])\n","torch.Size([4, 256, 160, 160])\n","torch.Size([4, 256, 160, 160])\n"]}]},{"cell_type":"markdown","source":["## 6.4 Head 교체"],"metadata":{"id":"CHwMcwJB6eWh"}},{"cell_type":"markdown","source":["```yaml\n","# @package _global_\n","\n","# https://arxiv.org/pdf/1911.08947.pdf 참조\n","\n","models:\n","  head:\n","    _target_: ${head_path}.DBHead\n","    in_channels: 1024                # 256\n","    upscale: 4                       # 4\n","    k: 50                            # The amplifying factor\n","    bias: False                      # Use bias or not in LayerNorm\n","    smooth: False                    # Use smooth or not in Upsample\n","    postprocess:\n","      thresh: 0.3                    # Binarization threshold\n","      box_thresh: 0.7                # Detection Box threshold\n","      max_candidates: 300            # Limit the number of detection boxes\n","      use_polygon: False             # Detection Box Type (QUAD or POLY)\n","```"],"metadata":{"id":"4YpyZs2j6jDD"}},{"cell_type":"code","source":["cfg.models.head.in_channels = 256 * 4\n","cfg.models.head.upscale = 4\n","\n","head = instantiate(cfg.models.head).to(device)\n","with torch.no_grad():\n","  head_output = head(decoder_features)\n","  for k, v in head_output.items():\n","      print(f'{k}: {v.shape}')"],"metadata":{"id":"ONuzBQi263wV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709025087672,"user_tz":-540,"elapsed":382,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"59b74bb4-dd1c-449e-aaf3-f9a79d8c4e53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["prob_maps: torch.Size([4, 1, 640, 640])\n","thresh_maps: torch.Size([4, 1, 640, 640])\n","binary_maps: torch.Size([4, 1, 640, 640])\n"]}]},{"cell_type":"code","source":["#@markdown ### 메모리 해제\n","\n","#@markdown - 미사용 자원을 반환하고 메모리를 확보합니다.\n","#@markdown ---\n","\n","import gc\n","\n","try:\n","  if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","\n","  del encoder\n","  del encoder_features\n","  del decoder\n","  del decoder_features\n","  del head\n","  del head_output\n","\n","  gc.collect()\n","\n","  print(\"Clear!\")\n","\n","except:\n","  pass"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"ONwgIDFZmU6x","executionInfo":{"status":"ok","timestamp":1709025090949,"user_tz":-540,"elapsed":365,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"9bdd5b40-5626-43ab-8610-d20baf3c78e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clear!\n"]}]},{"cell_type":"markdown","source":["## 6.5 Dataset augmentation"],"metadata":{"id":"u7b8BGEd7FnC"}},{"cell_type":"markdown","source":["```yaml\n","transforms:\n","  train_transform:\n","    _target_: ${dataset_path}.DBTransforms\n","    transforms:\n","      - _target_: albumentations.LongestMaxSize\n","        max_size: 640\n","        p: 1.0\n","      - _target_: albumentations.PadIfNeeded\n","        min_width: 640\n","        min_height: 640\n","        border_mode: 0\n","        p: 1.0\n","      - _target_: albumentations.HorizontalFlip\n","        p: 0.5\n","      - _target_: albumentations.Normalize\n","        mean: [0.485, 0.456, 0.406]\n","        std: [0.229, 0.224, 0.225]\n","      - _target_: albumentations.ShiftScaleRotate    # 추가된 augmentation\n","        shift_limit: 0.5\n","        scale_limit: (0.5, 1.0)\n","    keypoint_params:\n","      _target_: albumentations.KeypointParams\n","      format: 'xy'\n","      remove_invisible: True\n","```"],"metadata":{"id":"p2S-CA-x7Nmz"}},{"cell_type":"code","source":["cfg.transforms.train_transform.transforms.append(\n","    omegaconf.OmegaConf.create(\n","        dict(\n","            _target_='albumentations.ShiftScaleRotate',\n","            shift_limit=0.5,\n","            scale_limit=(0.5, 1.0)\n","        )\n","    )\n",")\n","train_dataset = instantiate(cfg.datasets.train_dataset)\n","\n","data = train_dataset[0]\n","data.keys()"],"metadata":{"id":"-x19vqEQ7OkG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709025094096,"user_tz":-540,"elapsed":306,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"ff96e576-e2d5-478b-8202-c13b130ce081"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['image', 'image_filename', 'shape', 'polygons', 'inverse_matrix'])"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["## 6.6 후처리 고도화"],"metadata":{"id":"i9-ZAMpc8Mo_"}},{"cell_type":"markdown","source":["```yaml\n","# @package _global_\n","\n","# https://arxiv.org/pdf/1911.08947.pdf 참조\n","\n","models:\n","  head:\n","    _target_: ${head_path}.DBHead\n","    in_channels: 256                 # Input layer channel\n","    upscale: 4                       # Output layer scale factor\n","    k: 50                            # The amplifying factor\n","    bias: False                      # Use bias or not in LayerNorm\n","    smooth: False                    # Use smooth or not in Upsample\n","    postprocess:\n","      thresh: 0.2                    # 0.3\n","      box_thresh: 0.3                # 0.4\n","      max_candidates: 1000           # 300\n","      use_polygon: True              # False\n","```"],"metadata":{"id":"Up3NFg8-8O3L"}},{"cell_type":"code","source":["cfg.models.head.postprocess.thresh = 0.2\n","cfg.models.head.postprocess.box_thresh = 0.3\n","cfg.models.head.postprocess.max_candidates = 1000\n","cfg.models.head.postprocess.use_polygon = True"],"metadata":{"id":"QZaBXnc_8ncH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6.7 Customize model\n","\n","Encoder를 예시로 Custom\n","\n","아래의 예시 클래스를 ocr/models/encoder/ 폴더 아래에 생성"],"metadata":{"id":"dNbWsU1F-cgI"}},{"cell_type":"code","source":["import torch\n","\n","def conv_layer(chann_in, chann_out, k_size, p_size):\n","    layer = torch.nn. Sequential(\n","        torch.nn.Conv2d(chann_in, chann_out, kernel_size=k_size, padding=p_size),\n","        torch.nn.BatchNorm2d(chann_out),\n","        torch.nn.ReLU()\n","    )\n","    return layer\n","\n","def vgg_conv_block(in_list, out_list, k_list, p_list, pooling_k, pooling_s):\n","    layers = [ conv_layer(in_list[i], out_list[i], k_list[i], p_list[i]) for i in range(len(in_list)) ]\n","    layers += [ torch.nn.MaxPool2d(kernel_size = pooling_k, stride = pooling_s) ]\n","    return torch.nn.Sequential(*layers)\n","\n","def vgg_fc_layer(size_in, size_out) :\n","    layer = torch.nn.Sequential(\n","        torch.nn.Linear(size_in, size_out),\n","        torch.nn.BatchNormld(size_out),\n","        torch.nn.ReLU()\n","    )\n","    return layer\n","\n","class VGG16(torch.nn.Module):\n","    def __init__(self):\n","        super(VGG16, self).__init__()\n","\n","        # Conv blocks (BatchNorm + ReLU activation added in each block)\n","        self.layer1 = vgg_conv_block([3,64], [64,64], [3,3], [1,1], 2, 2)\n","        self.layer2 = vgg_conv_block([64,128], [128,128], [3,3], [1,1], 2, 2)\n","        self.layer3 = vgg_conv_block([128,256,256], [256,256,256], [3,3,3], [1,1,1], 2, 2)\n","        self.layer4 = vgg_conv_block([256,512,512], [512,512,512], [3,3,3], [1,1,1], 2, 2)\n","        self.layer5 = vgg_conv_block([512,512,512], [512,512,512], [3,3,3], [1,1,1], 2, 2)\n","\n","    def forward(self, x):\n","        features = [self.layer1(x)]\n","        features.append(self.layer2(features[-1]))\n","        features.append(self.layer3(features[-1]))\n","        features.append(self.layer4(features[-1]))\n","        features.append(self.layer5(features[-1]))\n","\n","        return features"],"metadata":{"id":"mGK03Yap-pBL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vgg_encoder = VGG16().to(device)\n","vgg_output = vgg_encoder(data_loaded[\"images\"])\n","for v in vgg_output:\n","    print (v.shape)"],"metadata":{"id":"GqFxPdRGXvhl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709025104312,"user_tz":-540,"elapsed":260,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"d6b05b59-90dc-423f-d2a1-6ea5cdb75895"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 64, 320, 320])\n","torch.Size([4, 128, 160, 160])\n","torch.Size([4, 256, 80, 80])\n","torch.Size([4, 512, 40, 40])\n","torch.Size([4, 512, 20, 20])\n"]}]},{"cell_type":"code","source":["#@markdown ### 메모리 해제\n","\n","#@markdown - 미사용 자원을 반환하고 메모리를 확보합니다.\n","#@markdown ---\n","\n","import gc\n","from numba import cuda\n","\n","try:\n","  if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","\n","  del vgg_encoder\n","  del vgg_output\n","\n","  gc.collect()\n","\n","  print(\"Clear!\")\n","\n","except:\n","  pass"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"lTh4n74OlzY5","executionInfo":{"status":"ok","timestamp":1709025116641,"user_tz":-540,"elapsed":908,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"b176263c-1a1a-4881-c30c-f4ca31c51d87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clear!\n"]}]},{"cell_type":"code","source":["cfg.models.encoder._target_ = cfg.encoder_path + '.vgg16.VGG16'\n","del cfg.models.encoder.model_name\n","del cfg.models.encoder.select_features\n","del cfg.models.encoder.pretrained\n","\n","with torch.no_grad():\n","  encoder = instantiate(cfg.models.encoder).to(device)\n","  encoder_features = encoder(data_loaded[\"images\"])\n","  for encoder_feature in encoder_features:\n","      print(f'encoder feature shape: {encoder_feature.shape}')\n","\n","  cfg.models.decoder.in_channels = [64, 128, 256, 512, 512]\n","  cfg.models.decoder.strides = [2, 4, 8, 16, 32]\n","  decoder = instantiate(cfg.models.decoder).to(device)\n","  decoder_features = decoder(encoder_features)\n","  for decoder_feature in decoder_features:\n","      print(f'decoder feature shape: {decoder_feature.shape}')"],"metadata":{"id":"n1NPMY2zY9g8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709025134886,"user_tz":-540,"elapsed":796,"user":{"displayName":"Sangjoon Han","userId":"10245612847654687799"}},"outputId":"0017afe4-2ea7-4c23-ac71-02a1261ab8c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["encoder feature shape: torch.Size([4, 64, 320, 320])\n","encoder feature shape: torch.Size([4, 128, 160, 160])\n","encoder feature shape: torch.Size([4, 256, 80, 80])\n","encoder feature shape: torch.Size([4, 512, 40, 40])\n","encoder feature shape: torch.Size([4, 512, 20, 20])\n","decoder feature shape: torch.Size([4, 256, 320, 320])\n","decoder feature shape: torch.Size([4, 256, 320, 320])\n","decoder feature shape: torch.Size([4, 256, 320, 320])\n","decoder feature shape: torch.Size([4, 256, 320, 320])\n","decoder feature shape: torch.Size([4, 256, 320, 320])\n"]}]}]}